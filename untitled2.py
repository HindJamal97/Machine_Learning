# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p3Xgk2RXKVW4RO_wVCw6hmtgqeyWdPXS
"""

# if wget is not installed, we suggest to run also the following code.
!pip install wget

import wget

url = 'https://unimibox.unimi.it/index.php/s/eNGYGSYmqynNMqF/download'
filename = wget.download(url)
print(filename)

# if wget is not installed, we suggest to run also the following code.
!pip install unzip

import zipfile
with zipfile.ZipFile("CatsDogs.zip", 'r') as zip_ref:
    zip_ref.extractall(r'Downloads')

# if wget is not installed, we suggest to run also the following code.
!pip install sklearn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import warnings
warnings.filterwarnings('ignore')
import os
import tqdm
import random
import scipy
import cv2
from pathlib import Path
import tensorflow as tf

from tensorflow import keras
from keras import optimizers

from tensorflow.keras.utils import load_img
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import zero_one_loss

input_path=[]

label=[]

for class_name in os.listdir("/content/Downloads/CatsDogs"):
    for path in os.listdir("/content/Downloads/CatsDogs/"+class_name):
        if class_name == 'Cats':
            label.append(0)
        else:
            label.append(1)
        input_path.append(os.path.join("/content/Downloads/CatsDogs", class_name, path))
print(input_path[0], label[0])

print(input_path[10], label[10])

print(input_path[-10], label[-10])

len(input_path)

len(label)

df = pd.DataFrame()
df['images'] = input_path
df['label'] = label
df = df.sample(frac=1).reset_index(drop=True)
df.head()

df['images'].shape

#One important step is the clearing of the data, to avoid considering empty images or files that are nto images. 
for i in df['images']:
    if '.jpg' not in i:
        print(i)

#they are images that are actually not represented (there is no cat or no dog to be represented)
import PIL
l = []
for image in df['images']:
    try:
        img = PIL.Image.open(image)
    except:
        l.append(image)
l

# delete db files
df = df[df['images']!='/content/Downloads/CatsDogs/Dogs/11702.jpg']
df = df[df['images']!='/content/Downloads/CatsDogs/Cats/666.jpg']
len(df)

df['label']

import seaborn as sns
sns.catplot(x='label',data=df,kind="count")

fig = plt.gcf()
fig.set_size_inches(16, 16)
  
cat_dir = os.path.join('/content/Downloads/CatsDogs/Cats')
dog_dir = os.path.join('/content/Downloads/CatsDogs/Dogs')
cat_names = os.listdir(cat_dir)
dog_names = os.listdir(dog_dir)
  
pic_index = 210
  
cat_images = [os.path.join(cat_dir, fname)
              for fname in cat_names[pic_index-8:pic_index]]
dog_images = [os.path.join(dog_dir, fname)
              for fname in dog_names[pic_index-8:pic_index]]
  
for i, img_path in enumerate(cat_images + dog_images):
    sp = plt.subplot(4, 4, i+1)
    sp.axis('Off')
  
    img = mpimg.imread(img_path)
    plt.imshow(img)
  
plt.show()

df['label'] = df['label'].astype('str')  #this is useful for when we will need the values expressed as a string.

df.head()

df.tail()

train_df, test_df = train_test_split(df, train_size=0.8, shuffle=True, random_state=42)
train_df, val_df = train_test_split(train_df, test_size=0.1, shuffle=True, random_state=42)

train_df

test_df

val_df

total_train = train_df.shape[0]
total_test = test_df.shape[0]
total_validate = val_df.shape[0]

print('Train shape:')
print(train_df.shape)
print('Test shape')
print(test_df.shape)
print('Validation shape')
print(val_df.shape)

Image_Width=128
Image_Height=128
Image_Size=(Image_Width,Image_Height)
Image_Channels=3

from keras.preprocessing.image import ImageDataGenerator
train_generator = ImageDataGenerator(
    rescale = 1./255,  # normalization of images
    rotation_range = 10, # augmention of images to avoid overfitting
    shear_range = 0.1,
    zoom_range = 0.1,
    horizontal_flip = True,
)

test_generator = ImageDataGenerator(rescale = 1./255)

print("For the training set we") 
train_iterator = train_generator.flow_from_dataframe(
    train_df, 
    x_col='images', 
    y_col='label', 
    target_size=Image_Size, 
    color_mode= 'rgb',
    batch_size=32, 
    shuffle=True,
    class_mode='binary'   
)

print("For the test set we") 
test_iterator = test_generator.flow_from_dataframe(
    test_df, 
    x_col='images', 
    y_col='label', 
    target_size=Image_Size,
    color_mode= 'rgb',
    batch_size=32,     
    class_mode='binary',
    shuffle=False,
)

print("For the validation set we") 
validation_iterator = train_generator.flow_from_dataframe(
    val_df, 
    x_col='images', 
    y_col='label', 
    target_size=(Image_Size), 
    color_mode= 'rgb',
    batch_size=32, seed = 87, 
    shuffle=True,
    class_mode='binary'
)

print(test_iterator.class_indices)
print(test_iterator.n)
print(len(test_iterator))

example_df = train_df.sample(n=1).reset_index(drop=True)
example_generator = train_generator.flow_from_dataframe(
    example_df, 
    "../input/train/train/", 
    x_col='images', 
    y_col='label',
    class_mode='categorical'
)
plt.figure(figsize=(12, 12))
for i in range(0, 9):
    plt.subplot(3, 3, i+1)
    for X_batch, Y_batch in example_generator:
        image = X_batch[0]
        plt.imshow(image)
        break
plt.tight_layout()
plt.show()

from keras import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation
from keras.utils import plot_model
from keras.applications.mobilenet_v2 import MobileNetV2

#1VGG
model1 = Sequential([
                    Conv2D(32, (3,3), padding = "same", use_bias=False, activation='relu', input_shape=(Image_Width,Image_Height,3)),
                    Conv2D(32, (3,3), padding = "same", use_bias=False, activation='relu'),
                    MaxPooling2D(pool_size = (2,2)),
                    Flatten(),
                    Dense(128, use_bias=False, activation='relu'),
                    Dense(1, activation='sigmoid')
])

model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model1.summary()

plot_model(model1, show_shapes = True, expand_nested = True,dpi = 80)

!pip install visualkeras

import visualkeras
visualkeras.layered_view(model1, legend=True)

history = model1.fit(train_iterator, epochs=10, validation_data=validation_iterator, validation_steps=total_validate//32, steps_per_epoch=total_train//32)

results = model1.evaluate(test_iterator)

print("    Test Loss: {:.5f}".format(results[0]))
print("Test Accuracy: {:.2f}%".format(results[1] * 100))

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training Accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')
plt.title('Training and validation accuracy')
plt.ylabel('accuracy')  
plt.xlabel('epoch')
plt.legend()
plt.figure()


plt.plot(epochs, loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and validation loss')
plt.ylabel('loss')  
plt.xlabel('epoch')
plt.legend()
plt.show()

predictions = (model1.predict(test_iterator) >= 0.5).astype(np.int)

cm = confusion_matrix(test_iterator.labels, predictions, labels=[0, 1])
clr = classification_report(test_iterator.labels, predictions, labels=[0, 1], target_names=["CAT", "DOG"])

plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)
plt.xticks(ticks=[0.5, 1.5], labels=["CAT", "DOG"])
plt.yticks(ticks=[0.5, 1.5], labels=["CAT", "DOG"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

print("Classification Report:\n----------------------\n", clr)

cnn_loss=[]
zo_loss = zero_one_loss(test_iterator.labels, predictions)
cnn_loss.append(zo_loss)

print("Zero-one Loss: ", zo_loss)

cnn_loss

#2VGG
model2 = Sequential([
                    Conv2D(32, (3,3), padding = "same", use_bias=False, activation='relu', input_shape=(Image_Width,Image_Height,3)),
                    Conv2D(32, (3,3), padding = "same", use_bias=False, activation='relu'),
                    MaxPooling2D(pool_size = (2,2)),
                    Conv2D(64, (3,3), padding = "same", use_bias=False, activation='relu'),
                    Conv2D(64, (3,3), padding = "same", use_bias=False, activation='relu'),
                    MaxPooling2D(pool_size = (2,2)),
                    Flatten(),
                    Dense(128, use_bias=False, activation='relu'),
                    Dense(1, activation='sigmoid')
])

model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model2.summary()

plot_model(model2, show_shapes = True,expand_nested = True,dpi = 80)

visualkeras.layered_view(model2, legend=True)

history = model2.fit(train_iterator, epochs=10, validation_data=validation_iterator, validation_steps=total_validate//32, steps_per_epoch=total_train//32)

results = model2.evaluate(test_iterator)

print("    Test Loss: {:.5f}".format(results[0]))
print("Test Accuracy: {:.2f}%".format(results[1] * 100))

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training Accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')
plt.title('Training and validation accuracy')
plt.ylabel('accuracy')  
plt.xlabel('epoch')
plt.legend()
plt.figure()


plt.plot(epochs, loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and validation loss')
plt.ylabel('loss')  
plt.xlabel('epoch')
plt.legend()
plt.show()

predictions = (model2.predict(test_iterator) >= 0.5).astype(np.int)

cm = confusion_matrix(test_iterator.labels, predictions, labels=[0, 1])
clr = classification_report(test_iterator.labels, predictions, labels=[0, 1], target_names=["CAT", "DOG"])

plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)
plt.xticks(ticks=[0.5, 1.5], labels=["CAT", "DOG"])
plt.yticks(ticks=[0.5, 1.5], labels=["CAT", "DOG"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

print("Classification Report:\n----------------------\n", clr)

zo_loss2 = zero_one_loss(test_iterator.labels, predictions)
cnn_loss.append(zo_loss2)

print("Zero-one Loss: ", zo_loss2)

cnn_loss

#3VGG
model3 = Sequential([
                    Conv2D(32, (3,3), padding = "same", use_bias=False, activation='relu', input_shape=(Image_Width,Image_Height,3)),
                    Conv2D(32, (3,3), padding = "same", use_bias=False, activation='relu'),
                    MaxPooling2D(pool_size = (2,2)),
                    Conv2D(64, (3,3), padding = "same", use_bias=False, activation='relu'),
                    Conv2D(64, (3,3), padding = "same", use_bias=False, activation='relu'),
                    MaxPooling2D(pool_size = (2,2)),
                    Conv2D(128, (3,3), padding = "same", use_bias=False, activation='relu'),
                    Conv2D(128, (3,3), padding = "same", use_bias=False, activation='relu'),
                    MaxPooling2D(pool_size = (2,2)),
                    Flatten(),
                    Dense(128, use_bias=False, activation='relu'),
                    Dense(1, activation='sigmoid')
])

model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model3.summary()

plot_model(model3, show_shapes = True,expand_nested = True,dpi = 80)

visualkeras.layered_view(model3, legend=True)

history = model3.fit(train_iterator, epochs=10, validation_data=validation_iterator, validation_steps=total_validate//32, steps_per_epoch=total_train//32)

results = model3.evaluate(test_iterator)

print("    Test Loss: {:.5f}".format(results[0]))
print("Test Accuracy: {:.2f}%".format(results[1] * 100))

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training Accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')
plt.title('Training and validation accuracy')
plt.ylabel('accuracy')  
plt.xlabel('epoch')
plt.legend()
plt.figure()


plt.plot(epochs, loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and validation loss')
plt.ylabel('loss')  
plt.xlabel('epoch')
plt.legend()
plt.show()

predictions = (model3.predict(test_iterator) >= 0.5).astype(np.int)

cm = confusion_matrix(test_iterator.labels, predictions, labels=[0, 1])
clr = classification_report(test_iterator.labels, predictions, labels=[0, 1], target_names=["CAT", "DOG"])

plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)
plt.xticks(ticks=[0.5, 1.5], labels=["CAT", "DOG"])
plt.yticks(ticks=[0.5, 1.5], labels=["CAT", "DOG"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

print("Classification Report:\n----------------------\n", clr)

# ZERO ONE LOSS

zo_loss3 = zero_one_loss(test_iterator.labels, predictions)
cnn_loss.append(zo_loss3)

print("Zero-one Loss: ", zo_loss3)

cnn_loss

#3VGG with dropout and Bacht
model4 = Sequential([
                    Conv2D(32, (3,3), padding = "same", use_bias=False, activation='relu', input_shape=(Image_Width,Image_Height,3)),
                    BatchNormalization(),
                    Conv2D(32, (3,3), padding = "same", use_bias=False, activation='relu'),
                    BatchNormalization(),
                    MaxPooling2D(pool_size = (2,2)),
                    Dropout(0.25),
                    Conv2D(64, (3,3), padding = "same", use_bias=False, activation='relu'),
                    BatchNormalization(),
                    Conv2D(64, (3,3), padding = "same", use_bias=False, activation='relu'),
                    BatchNormalization(),
                    MaxPooling2D(pool_size = (2,2)),
                    Dropout(0.25),
                    Conv2D(128, (3,3), padding = "same", use_bias=False, activation='relu'),
                    BatchNormalization(),
                    Conv2D(128, (3,3), padding = "same", use_bias=False, activation='relu'),
                    BatchNormalization(),
                    MaxPooling2D(pool_size = (2,2)),
                    Dropout(0.25),
                    Flatten(),
                    Dense(128, use_bias=False, activation='relu'),
                    BatchNormalization(),
                    Dropout(0.5),
                    Dense(1, activation='sigmoid')
])

model4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model4.summary()

plot_model(model4, show_shapes = True,expand_nested = True,dpi = 80)

visualkeras.layered_view(model4, legend=True)

history = model4.fit(train_iterator, epochs=10, validation_data=validation_iterator, validation_steps=total_validate//32, steps_per_epoch=total_train//32)

results = model4.evaluate(test_iterator)

print("    Test Loss: {:.5f}".format(results[0]))
print("Test Accuracy: {:.2f}%".format(results[1] * 100))

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training Accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')
plt.title('Training and validation accuracy')
plt.ylabel('accuracy')  
plt.xlabel('epoch')
plt.legend()
plt.figure()


plt.plot(epochs, loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and validation loss')
plt.ylabel('loss')  
plt.xlabel('epoch')
plt.legend()
plt.show()

predictions = (model4.predict(test_iterator) >= 0.5).astype(np.int)

cm = confusion_matrix(test_iterator.labels, predictions, labels=[0, 1])
clr = classification_report(test_iterator.labels, predictions, labels=[0, 1], target_names=["CAT", "DOG"])

plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)
plt.xticks(ticks=[0.5, 1.5], labels=["CAT", "DOG"])
plt.yticks(ticks=[0.5, 1.5], labels=["CAT", "DOG"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

print("Classification Report:\n----------------------\n", clr)

zo_loss4 = zero_one_loss(test_iterator.labels, predictions)
cnn_loss.append(zo_loss4)

print("Zero-one Loss: ", zo_loss4)

cnn_loss

df_cnn_loss = pd.DataFrame(
columns = ['type', 'zero_one']
)

types = ["1VGG","2VGG","3VGG", "3VGG-drop-norm"]
df_cnn_loss = pd.DataFrame(data = [types,cnn_loss], index = ['types', 'zero_one']).T

df_cnn_loss

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from plotnine import *

# %matplotlib inline

!pip install ggplot

ggplot(df_cnn_loss, aes(x='types', y='zero_one',group=1)) + \
    geom_point() + \
    geom_line() + \
    theme_bw(base_size=12) + ggtitle("Zero-one loss CNN") + ylab("loss")

#Cross validation

from sklearn.model_selection import KFold
kf = KFold(n_splits = 5)
kf

i = 1
for train_df, test_df in kf.split(input_path, label):
    print("iteration ", i)
    print(train_df, " having :" , len(train_df))
    print(test_df, " having :" , len(test_df))
    print("-------------------------")
    i += 1

# Define per-fold score containers
acc_per_fold = []
loss_per_fold = []

# Define the K-fold Cross Validator
kfold = KFold(n_splits=5, shuffle=True)

# K-fold Cross Validation model evaluation
fold_no = 1
for train_df, test_df in kfold.split(input_path, label):

  # Define the model architecture
    model = Sequential([
                    Conv2D(32, (3,3), padding = "same", use_bias=False, activation='relu', input_shape=(Image_Width,Image_Height,3)),
                    BatchNormalization(),
                    Conv2D(32, (3,3), padding = "same", use_bias=False, activation='relu'),
                    BatchNormalization(),
                    MaxPooling2D(pool_size = (2,2)),
                    Dropout(0.25),
                    Conv2D(64, (3,3), padding = "same", use_bias=False, activation='relu'),
                    BatchNormalization(),
                    Conv2D(64, (3,3), padding = "same", use_bias=False, activation='relu'),
                    BatchNormalization(),
                    MaxPooling2D(pool_size = (2,2)),
                    Dropout(0.25),
                    Conv2D(128, (3,3), padding = "same", use_bias=False, activation='relu'),
                    BatchNormalization(),
                    Conv2D(128, (3,3), padding = "same", use_bias=False, activation='relu'),
                    BatchNormalization(),
                    MaxPooling2D(pool_size = (2,2)),
                    Dropout(0.25),
                    Flatten(),
                    Dense(128, use_bias=False, activation='relu'),
                    BatchNormalization(),
                    Dropout(0.5),
                    Dense(1, activation='sigmoid')
])

  # Compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


  # Generate a print
    print('------------------------------------------------------------------------')
    print(f'Training for fold {fold_no} ...')

  # Fit data to model
    history = model.fit(train_iterator, epochs=10, validation_data=validation_iterator, validation_steps=total_validate//32, steps_per_epoch=total_train//32)

  # Generate generalization metrics
    scores = model.evaluate(test_iterator)
    predictions = (model.predict(test_iterator) >= 0.5).astype(np.int)
    zero_oneloss = zero_one_loss(test_iterator.labels, predictions)
    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {zero_oneloss}; {model.metrics_names[1]} of {scores[1]*100}%')
    acc_per_fold.append(scores[1] * 100)
    loss_per_fold.append(zero_oneloss*100)
    

  # Increase fold number
    fold_no = fold_no + 1

# == Provide average scores ==
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)}')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')

#Additional approaches!! MobileNetV2

base_model = MobileNetV2(input_shape=(Image_Width,Image_Height,3),
                                               include_top=False,
                                               weights='imagenet')

base_model.summary()

base_model.trainable = False

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
prediction_layer = tf.keras.layers.Dense(1)

Model = Sequential([
    base_model,
    global_average_layer,
    prediction_layer
])

base_learning_rate = 0.0001
Model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=base_learning_rate),
              loss=keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

initial_epochs = 10
validation_steps=20

loss0,accuracy0 = Model.evaluate(test_iterator, steps = validation_steps)

history = Model.fit(train_iterator,
                    epochs = initial_epochs,
                    validation_data=validation_iterator)
acc = history.history['accuracy']
print(acc)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Training', 'Validation'])
plt.grid()
plt.show()

(loss, accuracy) = Model.evaluate(test_iterator)
print(f'Loss: {loss}')
print(f'Accuracy: {accuracy}')

predictions = (Model.predict(test_iterator) >= 0.5).astype(np.int)

cm = confusion_matrix(test_iterator.labels, predictions, labels=[0, 1])
clr = classification_report(test_iterator.labels, predictions, labels=[0, 1], target_names=["CAT", "DOG"])

plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)
plt.xticks(ticks=[0.5, 1.5], labels=["CAT", "DOG"])
plt.yticks(ticks=[0.5, 1.5], labels=["CAT", "DOG"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

print("Classification Report:\n----------------------\n", clr)

ZO_loss = zero_one_loss(test_iterator.labels, predictions)

print("Zero-one Loss: ", ZO_loss)

# Define per-fold score containers
acc_per_fold = []
loss_per_fold = []

# Define the K-fold Cross Validator
kfold = KFold(n_splits=5, shuffle=True)

# K-fold Cross Validation model evaluation
fold_no = 1
for train_df, test_df in kfold.split(input_path, label):

# Define the model architecture
    base_model = MobileNetV2(input_shape=(Image_Width,Image_Height,3),
                                               include_top=False,
                                               weights='imagenet')
    base_model.trainable = False

    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
    prediction_layer = tf.keras.layers.Dense(1)

    Model = Sequential([base_model,global_average_layer,prediction_layer
]) 


  # Compile the model
    base_learning_rate = 0.0001
    Model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=base_learning_rate),
              loss=keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])


  # Generate a print
    print('------------------------------------------------------------------------')
    print(f'Training for fold {fold_no} ...')

  # Fit data to model
    history = Model.fit(train_iterator,epochs = initial_epochs,validation_data=validation_iterator)

  # Generate generalization metrics
    scores = Model.evaluate(test_iterator)
    predictions = (Model.predict(test_iterator) >= 0.5).astype(np.int)
    zero_oneloss = zero_one_loss(test_iterator.labels, predictions)
    print(f'Score for fold {fold_no}: {Model.metrics_names[0]} of {zero_oneloss}; {Model.metrics_names[1]} of {scores[1]*100}%')
    acc_per_fold.append(scores[1] * 100)
    loss_per_fold.append(zero_oneloss*100)
    

  # Increase fold number
    fold_no = fold_no + 1

# == Provide average scores ==
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)}')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')

## Proof

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import tensorflow
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
import numpy as np

img1 = image.load_img('/content/Downloads/CatsDogs/Cats/2546.jpg', target_size=(128, 128))
img = image.img_to_array(img1)
img = img/255
img = np.expand_dims(img, axis=0)
prediction = Model.predict(img, batch_size=32,steps=1)
if(prediction[:,:]>0.5):
    value ='Dog'%(prediction[0,0])
    plt.text(20, 62,value,color='red',fontsize=18,bbox=dict(facecolor='white',alpha=0.8))
else:
    value ='Cat'%(1.0-prediction[0,0])
    plt.text(20, 62,value,color='red',fontsize=18,bbox=dict(facecolor='white',alpha=0.8))

plt.imshow(img1)
plt.show()